import pandas as pd
import jieba
from collections import defaultdict

# åŠ è½½æ•°æ®
def load_comments(csv_path):
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
    except UnicodeDecodeError:
        df = pd.read_csv(csv_path, encoding='gbk')
    
    # æ•°æ®æ¸…æ´—
    df['è¯„è®º'] = df['è¯„è®º'].str.replace(r'$$\\w\+?\_?\\d\+$$', '[è¡¨æƒ…]', regex=True)  # ç»Ÿä¸€ç‰¹æ®Šè¡¨æƒ…ç¬¦å·
    return df

# æƒ…æ„Ÿåˆ†æå¢å¼ºæ¨¡å—
class SentimentAnalyzer:
    def __init__(self):
        self.lexicon = self._load_lexicon()
        self.sentiment_map = {
            'positive': {'æƒé‡': 1.2, 'keywords': {
                '[æ”¯æŒ]', '[doge]', 'è‡´æ•¬', 'åšæŒ', 'å¯å‘', 'ä¼Ÿå¤§', 'ä¸“ä¸š', 'æ„Ÿè°¢',
                'æœ‰ç”¨', 'æ”¯æŒ', 'ç†è§£', 'å­¦ä¹ ', 'åº†å¹¸', 'æœ‰å¸®åŠ©', 'å–œæ¬¢', 'è‡´æ•¬'
            }},
            'negative': {'æƒé‡': 1.5, 'keywords': {
                '[æ— è¯­]', '[å¤§å“­]', 'çŒå¥‡', 'ç‚¸è£‚', 'æ¶å¿ƒ', 'åæ‚”', 'å°äº†', 'éª‚',
                'éª—', 'ç¼–é€ ', 'ä¸ç†è§£', 'å±é™©', 'ç—›è‹¦', 'ä¼¤å®³', 'å˜æ€', 'å¼‚å¸¸'
            }},
            'neutral': {'æƒé‡': 1.0, 'keywords': {
                '[ç¬‘å“­]', 'ğŸ¤“', 'ç¬¬ä¸€æ¬¡', 'æ˜¯ä¸æ˜¯', 'æ€ä¹ˆ', 'å¯èƒ½', 'æ„Ÿè§‰',
                'å»ºè®®', 'è®¨è®º', 'ç–‘é—®', 'æ€è€ƒ', 'åˆ†æ', 'çŒœæµ‹'
            }}
        }
        self.negation_words = {'ä¸', 'æ²¡', 'æ— ', 'é', 'æœª', 'è«'}  # å¦å®šè¯åˆ—è¡¨
        
        # ä¿®æ­£åˆ†è¯è¯å…¸åŠ è½½æ–¹å¼
        for sentiment in self.sentiment_map.values():
            for word in sentiment['keywords']:  # éå†æ¯ä¸ªå…³é”®è¯
                jieba.add_word(word)  # æ­£ç¡®çš„æ–¹æ³•è°ƒç”¨

    def _load_lexicon(self):
        """åŠ è½½Bç«™ä¸“ç”¨æƒ…æ„Ÿè¯å…¸"""
        lexicon = defaultdict(set)
        lexicon.update({
            'positive': {'yyds', 'ç ´é˜²äº†', 'æ³ªç›®', 'ä¸‰è¿'},
            'negative': {'èšŒåŸ ä½äº†', 'ä¸‹å¤´', 'å¯„', 'å…¸æ€¥å­'},
            'neutral': {'è°œè¯­äºº', 'é’è§’', 'å…¸', 'å­'}
        })
        return lexicon

    def _contains_negation(self, words, index):
        """æ£€æŸ¥å¦å®šä¿®é¥° (å¤„ç†ç±»ä¼¼"ä¸ä¸“ä¸š"çš„æƒ…å†µ)"""
        return index > 0 and words[index-1] in self.negation_words

    def analyze(self, text):
        words = list(jieba.cut(text))
        scores = defaultdict(float)

        for i, word in enumerate(words):
            for sentiment, data in self.sentiment_map.items():
                if word in data['keywords']:
                    modifier = 0.5 if self._contains_negation(words, i) else 1.0
                    scores[sentiment] += data['æƒé‡'] * modifier
            # å¤„ç†Bç«™ä¸“ç”¨è¯æ±‡
            for sentiment, terms in self.lexicon.items():
                if word in terms:
                    scores[sentiment] += 1.0

        # ç‰¹æ®Šè¡¨æƒ…ç¬¦å·å¤„ç†
        if '[è¡¨æƒ…]' in text:
            scores['neutral'] += 1.2

        if not scores:
            return 'ä¸­æ€§'
        
        max_sentiment = max(scores, key=scores.get)
        return 'æ­£é¢' if max_sentiment == 'positive' else \
               'è´Ÿé¢' if max_sentiment == 'negative' else 'ä¸­æ€§'

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    comments_df = load_comments("bilibili/test_comments_data.csv")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = SentimentAnalyzer()
    
    # æ‰§è¡Œåˆ†æ
    comments_df['æƒ…æ„Ÿ'] = comments_df['è¯„è®º'].apply(analyzer.analyze)
    
    # ä¿å­˜ç»“æœ
    # comments_df.to_csv("bilibili/analyzed_comments.csv", index=False, encoding='utf-8-sig')
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    report = comments_df['æƒ…æ„Ÿ'].value_counts(normalize=True).apply(lambda x: f"{x:.1%}")
    print(f"\næƒ…æ„Ÿåˆ†å¸ƒåˆ†æï¼š\n{report.to_markdown()}")