# -*- coding: utf-8 -*-
import torch
from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, pipeline
import pandas as pd
import re
import os

class WindowsBatchAnalyzer:
    def __init__(self):
        # ä½¿ç”¨é‡åŒ–æ¨¡å‹é™ä½æ˜¾å­˜å ç”¨
        self.model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"

        # åˆå§‹åŒ–æ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        
        # é‡åŒ–é…ç½®ä¼˜åŒ–
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,  # æ˜¾å¼æŒ‡å®šè®¡ç®—ç±»å‹
            bnb_4bit_quant_type="nf4",  # ä½¿ç”¨æœ€æ–°é‡åŒ–æ ¼å¼
            bnb_4bit_use_double_quant=True  # å¯ç”¨äºŒæ¬¡é‡åŒ–
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map="auto",
            torch_dtype=torch.float16,  # ä¿æŒæ•°æ®ç±»å‹ä¸€è‡´
            quantization_config=quantization_config,  # æ›¿æ¢æ—§å‚æ•°
            low_cpu_mem_usage=True,
            max_memory={0: "28GiB"}
        )

        # æ‰¹å¤„ç†ä¼˜åŒ–å‚æ•°
        self.pipeline = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            batch_size=32  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
        )

        # ä¼˜åŒ–åçš„æ¨¡æ¿
        self.prompt_template = """[INST] <<SYS>>
ä½œä¸ºèµ„æ·±äº§å“åˆ†æå¸ˆï¼Œè¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ‡å‡†åˆ†ç±»è¯„è®ºï¼š

ã€æ­£é¢ã€‘éœ€æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
âœ“ æ˜ç¡®è´­ä¹°æ„å‘ï¼š"å¿…ä¹°/é¦–å‘/å·²é¢„è´­"ç­‰
âœ“ ç§¯æè¯„ä»·ï¼š"å¤ªæ£’äº†/å€¼å¾—å…¥æ‰‹"ç­‰
âœ“ ç¤ºä¾‹ï¼š"å­˜é’±ç­‰å‘å”®ï¼" "ç”»è´¨æå‡æ˜æ˜¾"

ã€è´Ÿé¢ã€‘éœ€æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
âœ— ä»·æ ¼æŠ±æ€¨ï¼š"å¤ªè´µ/ä¸å€¼/å‰²éŸ­èœ"ç­‰
âœ— è´¨é‡é—®é¢˜ï¼š"åšå·¥å·®/æ€§èƒ½æ‹‰èƒ¯"ç­‰
âœ“ ç¤ºä¾‹ï¼š"ä»·æ ¼åŠé€€" "joyconåˆæ¼‚ç§»"

ã€ä¸­æ€§ã€‘æ— æ˜ç¡®æƒ…æ„Ÿå€¾å‘ï¼š
â€¢ å‚æ•°è®¨è®ºï¼š"CPUé¢‘ç‡/TFLOPSæ•°å€¼"
â€¢ æ™®é€šæé—®ï¼š"ä»€ä¹ˆæ—¶å€™å‘å”®"
<</SYS>>

è¯·åˆ†æè¯¥è¯„è®ºï¼ˆä¿æŒåŸå§‹è¡¨è¿°ï¼‰ï¼š
ã€Œ{comment}ã€

åªéœ€è¾“å‡ºã€æ­£é¢/ä¸­æ€§/è´Ÿé¢ã€‘ï¼š
[/INST] ã€"""

    def batch_analyze(self, texts):
        # æ‰¹é‡é¢„å¤„ç†
        cleaned = [self._clean_text(t) for t in texts]
        prompts = [self.prompt_template.format(comment=t[:512]) for t in cleaned]
        
        # æ‰¹é‡ç”Ÿæˆï¼ˆWindowsä¼˜åŒ–å‚æ•°ï¼‰
        outputs = self.pipeline(
            prompts,
            max_new_tokens=20,
            temperature=0.7,
            top_k=40,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id,
            num_return_sequences=1
        )
        
        return [self._parse(o[0]['generated_text']) for o in outputs]

    def _clean_text(self, text):
        # Windowsè·¯å¾„å…¼å®¹å¤„ç†
        return re.sub(r'($$.\*?$$|\\n|\\r)', '', text)

    def _parse(self, generated_text):
        # é«˜æ•ˆè§£æé€»è¾‘
        if "è´­ä¹°æ„æ„¿" in generated_text:
            return "è´­ä¹°æ„æ„¿"
        if "è´Ÿé¢è¯„ä»·" in generated_text:
            return "è´Ÿé¢è¯„ä»·"
        return "ä¸­æ€§è®¨è®º"

def windows_optimized_analysis(csv_path):
    # è·¯å¾„å…¼å®¹å¤„ç†
    df = pd.read_csv(csv_path, engine='python')
    
    # åˆ†æ‰¹å¤„ç†é˜²æ­¢å†…å­˜æº¢å‡º
    analyzer = WindowsBatchAnalyzer()
    batch_size = 32
    
    results = []
    for i in range(0, len(df), batch_size):
        batch = df['è¯„è®ºå†…å®¹'].iloc[i:i+batch_size].tolist()
        batch_results = analyzer.batch_analyze(batch)
        results.extend(batch_results)
        print(f"è¿›åº¦: {min(i+batch_size, len(df))}/{len(df)}")
    
    df['æƒ…æ„Ÿåˆ†ç±»'] = results

    def display_statistics(df):
        # ç»Ÿè®¡å„åˆ†ç±»æ•°é‡
        stats = df['æƒ…æ„Ÿåˆ†ç±»'].value_counts().reset_index()
        stats.columns = ['åˆ†ç±»', 'æ•°é‡']
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        total = len(df)
        stats['å æ¯”'] = (stats['æ•°é‡'] / total * 100).round(1).astype(str) + '%'
        
        # æ‰“å°ç»Ÿè®¡è¡¨æ ¼
        print("\nğŸ“Š æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡:")
        print(stats.to_markdown(index=False, tablefmt="grid", stralign="center"))
        
        # æ‰“å°éšæœºæ ·æœ¬
        print("\nğŸ” éšæœºæŠ½æ ·ç¤ºä¾‹:")
        sample = df.sample(5, random_state=42)[['è¯„è®ºå†…å®¹', 'æƒ…æ„Ÿåˆ†ç±»']]
        print(sample.to_markdown(index=False, tablefmt="grid", stralign="left"))

    # æ‰§è¡Œç»Ÿè®¡å±•ç¤º
    display_statistics(df)

    save_path = os.path.normpath(r"D:\\GitHubRepos\\is6941-ml-social-media\\analysis\\results\\comment_results.csv")
    df.to_csv(save_path, index=False, encoding='utf_8_sig')

if __name__ == "__main__":
    csv_path = os.path.normpath(r"analysis\data\\cleaned_BV1dZwLeKEzG_comments.csv")
    windows_optimized_analysis(csv_path)