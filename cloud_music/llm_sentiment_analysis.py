# -*- coding: utf-8 -*-
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import re
import pandas as pd
import os

class AnimeSentimentAnalyzer:
    def __init__(self):
        # æ¨¡å‹é€‰æ‹©(æ ¹æ®è‡ªå·±ç”µè„‘çš„æ˜¾å­˜é‡åŠ›è€Œè¡Œ)
        # macbookä½¿ç”¨ç»Ÿä¸€å†…å­˜æ¶æ„ï¼Œå› æ­¤å‚è€ƒå†…å­˜ï¼Œè€Œä¸æ˜¯æ˜¾å­˜
        self.model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
        # self.model_name = "google-bert/bert-base-uncased" # çƒ‚

        
        # åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        """ä¸€èˆ¬ç”¨ä»¥ä¸‹ä»£ç """
        # self.model = AutoModelForCausalLM.from_pretrained(
        #     self.model_name,
        #     trust_remote_code=True,
        #     torch_dtype=torch.bfloat16,
        #     device_map="auto",
        #     low_cpu_mem_usage=True
        # ).eval()
        """macbookç”¨ä»¥ä¸‹ä»£ç """
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="mps" if torch.backends.mps.is_available() else "auto",
            low_cpu_mem_usage=True
        ).to('mps').eval()
        
        # é¢†åŸŸæç¤ºæ¨¡æ¿
        self.prompt_template = """[INST] <<SYS>>
ä½ æ˜¯è§‚çœ‹è¿‡ã€ŠBanG Dream! It's MyGO!!!!!ã€‹ä¸€åƒéçš„èµ„æ·±åŠ¨æ¼«çˆ±å¥½è€…ã€‚ç°æœ‰200+æ¡æ¥è‡ªç½‘æ˜“äº‘çš„ã€Šæ˜¥æ—¥å½±ã€‹æ­Œæ›²è¯„è®ºï¼Œè¯·æŒ‰ä»¥ä¸‹è§„åˆ™åˆ†æè¯„è®ºæƒ…æ„Ÿï¼š
1. å¿…é¡»ç”¨ã€æ­£é¢/è´Ÿé¢/ä¸­æ€§ã€‘å¼€å¤´
2. åˆ¤æ–­æ ‡å‡†ï¼š
    - æ­£é¢ï¼šå‡ºç°å¥½è¯„è¯ï¼ˆç¥æ›²ã€æ³ªç›®ã€å¥½å¬ï¼‰æˆ–æ„ŸåŠ¨è¡¨æƒ…ï¼ˆğŸ˜­ã€ğŸ¸ï¼‰æˆ–å–„æ„ç©æ¢—
    - è´Ÿé¢ï¼šå‡ºç°å·®è¯„è¯ï¼ˆéš¾å¬ã€è¿·æƒ‘ã€åŠé€€ï¼‰æˆ–è´Ÿé¢è¡¨æƒ…ï¼ˆğŸ¤®ï¼‰
    - ä¸­æ€§ï¼šå…¶ä»–æƒ…å†µ
3. ç¤ºä¾‹ï¼š
    "æ˜¥æ—¥å½±å‰ä»–soloå¤ªæ£’äº†ï¼ğŸ˜­" â†’ æ­£é¢
    "æ¯é¸¡å¡å‰§æƒ…å¤ªè¿·æƒ‘äº†" â†’ è´Ÿé¢
    "å‘¨ä¸‰æ›´æ–°å—ï¼Ÿ" â†’ ä¸­æ€§
<</SYS>>

ç«‹å³åˆ†æè¯¥è¯„è®ºï¼š
"{comment}"
[/INST]
æƒ…æ„Ÿåˆ†ç±»ï¼šã€"""
        self.anime_terms = [
            'æ˜¥æ—¥å½±', 'mygo', 'mujica', 'soyo', 'ç¥¥å­', 'crychic', 
            'ave mujica', 'æ¯é¸¡å¡', 'ç¯', 'ç«‹å¸Œ', 'æ˜¥æ—¥å½±äº‹å˜'
        ]

    def preprocess(self, text):
        """æ–‡æœ¬æ¸…æ´—"""
        text = re.sub(r'\$\$\\w\+\?\$\$', '[è¡¨æƒ…]', text)
        return text[:1024]  # æ§åˆ¶è¾“å…¥é•¿åº¦

    def analyze(self, text):
        # æ–°å¢è®¾å¤‡çŠ¶æ€è¾“å‡º
        print(f"ğŸ”¥ æ˜¾å­˜å ç”¨: {torch.mps.current_allocated_memory()/1024**2:.2f} MB" 
            if torch.backends.mps.is_available() else "â³ CPUæ¨¡å¼è¿è¡Œ")

        # å‰ç½®è§„åˆ™è¦†ç›–
        text_lower = text.lower()
        
        # ç‰¹æ®Šå¥å¼è§„åˆ™
        if any(rule in text_lower for rule in ['å¤ªå¥½å¬', 'ç¥æ›²é¢„å®š', 'æ³ªç›®äº†']):
            return "æ­£é¢"
        if any(rule in text_lower for rule in ['éš¾å¬æ­»äº†', 'åŠé€€']):
            return "è´Ÿé¢"
        
        # ç‰¹å®šç»„åˆæ£€æµ‹
        if ('æ˜¥æ—¥å½±' in text) and ('åŒå‰ä»–' in text or 'ä¸ºä»€ä¹ˆ' in text):
            return "æ­£é¢"
        if 'å‘œå‘œå‘œ' in text and ('æ„ŸåŠ¨' in text or 'å“­' in text):
            return "æ­£é¢"

        """æ ¸å¿ƒåˆ†ææ–¹æ³•"""
        cleaned_text = self.preprocess(text)
        prompt = self.prompt_template.format(
            terms="ã€".join(self.anime_terms),
            comment=cleaned_text
        )
        """ä¸€èˆ¬ç”¨ä»¥ä¸‹ä»£ç """
        # inputs = self.tokenizer(
        #     prompt,
        #     return_tensors="pt"
        # ).to(self.model.device)
        """macbookç”¨ä»¥ä¸‹ä»£ç """
        device = 'mps' if torch.backends.mps.is_available() else 'cpu'
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt"
        ).to(device)
        
        """ä¸€èˆ¬ç”¨ä»¥ä¸‹ä»£ç """
        # with torch.no_grad():
        #     outputs = self.model.generate(
        #         **inputs,
        #         max_new_tokens=30,  # é™åˆ¶è¾“å‡ºé•¿åº¦
        #         temperature=0.9,    # å¢åŠ åˆ›é€ æ€§
        #         top_k=50,
        #         num_return_sequences=1,
        #         pad_token_id=self.tokenizer.eos_token_id,
        #         eos_token_id=self.tokenizer.convert_tokens_to_ids(["<|im_end|>"])[0]
        #     )
        """macbookç”¨ä»¥ä¸‹ä»£ç """
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=30,  # é™åˆ¶è¾“å‡ºé•¿åº¦
                temperature=0.9,    # å¢åŠ åˆ›é€ æ€§
                top_k=50,
                num_return_sequences=1,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.convert_tokens_to_ids(["<|im_end|>"])[0],
                # MPSä¸“å±ä¼˜åŒ–å‚æ•°
                use_cache=True,
                do_sample=True if torch.backends.mps.is_available() else False
            )
        
        response = self.tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
        return self._parse_response(response)

    def _parse_response(self, response):
        """å¢å¼ºç‰ˆè§£æé€»è¾‘"""
        # å¼ºåˆ¶æ ¼å¼åŒ¹é…
        if re.match(r'^æ­£é¢', response):
            return "æ­£é¢"
        if re.match(r'^è´Ÿé¢', response):
            return "è´Ÿé¢"
        
        # è¡¨æƒ…ç¬¦å·æ£€æµ‹
        emoji_rules = {
            'ğŸ˜­': 'æ­£é¢',
            'ğŸ¸': 'æ­£é¢',
            'ğŸ¤®': 'è´Ÿé¢'
        }
        for emo, label in emoji_rules.items():
            if emo in response:
                return label
        
        # å…³é”®è¯å¼ºåŒ–
        positive_keywords = {'å¥½å¬', 'è±ªå¬', 'ç¥æ›²', 'æ³ªç›®', 'æ„ŸåŠ¨', 'å–œæ¬¢', 'æ£’', 'çˆ½'}
        negative_keywords = {'éš¾å¬', 'è¿·æƒ‘', 'åŠé€€', 'åƒåœ¾', 'å¤±æœ›', 'çŒå¥‡', 'ä½ä¿—', 'ç¾éš¾'}
        
        text = response.lower()
        if any(kw in text for kw in positive_keywords):
            return "æ­£é¢"
        elif any(kw in text for kw in negative_keywords):
            return "è´Ÿé¢"
        
        # é‡å¤å­—ç¬¦æ£€æµ‹ï¼ˆå¦‚"å‘œå‘œå‘œå‘œ"è¡¨ç¤ºæ„ŸåŠ¨ï¼‰
        if re.search(r'(.)\1{3,}', text):  # è¿ç»­4ä¸ªç›¸åŒå­—ç¬¦
            return "æ­£é¢" if 'å¥½' in text else "è´Ÿé¢"
        
        return "ä¸­æ€§"

def read_comments(file_path):
    """è¯»å–è¯„è®ºæ–‡ä»¶"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ {file_path} æœªæ‰¾åˆ°")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]

def display_results(results):
    """ä¼˜åŒ–åçš„ç»“æœæ˜¾ç¤º"""
    df = pd.DataFrame(results)
    
    # æƒ…æ„Ÿç»Ÿè®¡
    stats = df['æƒ…æ„Ÿ'].value_counts().reset_index()
    stats.columns = ['æƒ…æ„Ÿç±»å‹', 'æ•°é‡']
    stats['å æ¯”'] = (stats['æ•°é‡'] / len(df) * 100).round(1).astype(str) + '%'
    
    # ç¤ºä¾‹å±•ç¤º
    samples = df.sample(5, random_state=42)
    
    # æ§åˆ¶å°è¾“å‡º
    print("\nğŸ¯ åˆ†æç»“æœç»Ÿè®¡")
    print(stats.to_markdown(index=False, tablefmt="grid", stralign="center"))
    
    print("\nğŸ” éšæœºæŠ½æ ·ç¤ºä¾‹")
    print(samples.to_markdown(index=False, tablefmt="grid", stralign="left"))
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    df.to_csv("cloud_music/sentiment_results.csv", index=False, encoding='utf_8_sig')
    print("\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜è‡³ sentiment_results.csv")

if __name__ == "__main__":
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = AnimeSentimentAnalyzer()
    
    try:
        # è¯»å–è¯„è®ºæ–‡ä»¶
        comments = read_comments("cloud_music/Haruhikage.txt")
        print(f"âœ… æˆåŠŸè¯»å– {len(comments)} æ¡è¯„è®º")
        
        # æ‰§è¡Œåˆ†æ
        results = []
        for comment in tqdm(comments, desc="åˆ†æè¿›åº¦", unit="æ¡"):
            results.append({
                "è¯„è®ºå†…å®¹": comment,
                "æƒ…æ„Ÿ": analyzer.analyze(comment)
            })
        
        # æ˜¾ç¤ºä¼˜åŒ–åçš„ç»“æœ
        display_results(results)
        
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")